{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B65pRSUNNRT2",
        "outputId": "8dd9442a-c2e9-4aa0-bf51-3573d1cc54e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and: 7\n",
            "of: 5\n",
            "to: 5\n",
            "we: 5\n",
            "letters: 3\n",
            "that: 3\n",
            "a: 3\n",
            "them: 3\n",
            "the: 3\n",
            "poems: 2\n",
            "they: 2\n",
            "have: 2\n",
            "context: 2\n",
            "their: 2\n",
            "own: 2\n",
            "read: 2\n",
            "are: 2\n",
            "our: 2\n",
            "epistolary: 2\n",
            "itâ€™s: 2\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "def mapper(line):\n",
        "    words = line.split()\n",
        "    return [(word.lower().strip(\".,!?;:()[]{}\\\"'\"), 1) for word in words]\n",
        "\n",
        "def reducer(word_counts):\n",
        "    return word_counts.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "def run_word_count(file_path):\n",
        "    spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
        "    sc = spark.sparkContext\n",
        "\n",
        "    # Read the file into an RDD\n",
        "    rdd = sc.textFile(file_path)\n",
        "\n",
        "    # Map and reduce steps\n",
        "    mapped_rdd = rdd.flatMap(mapper)\n",
        "    reduced_rdd = reducer(mapped_rdd)\n",
        "\n",
        "    # Collect and print the results\n",
        "    word_counts = reduced_rdd.collect()\n",
        "    for word, count in sorted(word_counts, key=lambda x: -x[1])[:20]:  # Display top 20 words\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/poem.txt\"\n",
        "    run_word_count(file_path)\n",
        "\n"
      ]
    }
  ]
}